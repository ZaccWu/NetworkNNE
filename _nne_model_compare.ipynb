{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from Positive_transform import Positive_transform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from normalRegressionLayer import NormalRegressionLayer\n",
    "import pickle\n",
    "from Test_error_summary import Test_error_summary\n",
    "import sys\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = argparse.ArgumentParser('nneTrain')\n",
    "# neural network algorithm/training settings\n",
    "parser.add_argument('--num_nodes', type=int, help='layer width', default=128)   # 128\n",
    "parser.add_argument('--batch_size', type=int, help='training sample batch', default=64)    # 64\n",
    "parser.add_argument('--max_epochs', type=int, help='training epoches', default=200)         # 200\n",
    "parser.add_argument('--initial_lr', type=int, help='initial learning rate', default=0.01)   # 0.01\n",
    "\n",
    "# display settings\n",
    "parser.add_argument('--disp_test_summary', type=bool, help='display test summary', default=True)\n",
    "parser.add_argument('--display_fig', type=bool, help='display figure', default=True)\n",
    "parser.add_argument('--disp_iter', type=bool, help='display iteration', default=True)\n",
    "parser.add_argument('--learn_standard_error', type=bool, help='learn standard error', default=True)\n",
    "\n",
    "parser.add_argument('-f', type=str, default=None, \n",
    "                    help=argparse.SUPPRESS)  # SUPPRESS 让它不出现在帮助文档中\n",
    "\n",
    "try:\n",
    "    args = parser.parse_args()\n",
    "except:\n",
    "    parser.print_help()\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_train_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def forward_loss(Y, T): # y_pred, y_true\n",
    "    #print(Y.shape)\n",
    "    #print(T.shape)\n",
    "    k = Y.shape[1] // 2\n",
    "    n = Y.shape[0]\n",
    "    U = Y[:, :k]\n",
    "    S = torch.tensor(Positive_transform(Y[:, k:2*k].detach().numpy()))\n",
    "    X = T[:, :k]\n",
    "    squared_err = 2 * torch.log(S) + ((U - X) / S) ** 2\n",
    "    # sum all elements, then divide by n\n",
    "    loss = squared_err.sum() / n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('simu_data_collect/model2/training_set_gen.pkl', 'rb') as f:\n",
    "#with open('training_set_gen.pkl', 'rb') as f:  # data from \"set_up.py\"\n",
    "    data = pickle.load(f)\n",
    "set_train_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load training/validation/real data\n",
    "input_train, label_train = data['input_train'], data['label_train']\n",
    "input_test, label_test = data['input_test'], data['label_test']\n",
    "input_real = data['input_real']\n",
    "lb, ub = data['lb'], data['ub']\n",
    "label_name = data['label_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 31) (100, 31) (31,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Moment list:\n",
    "1. (mean_deg, var_deg, clusterglobal)*period,\n",
    "2. (mean(y_ijt, x_ij,t-1), cov(y_ijt, x_ij,t-1))*period,\n",
    "x_ij,t-1 = (y_ij,t-1, SumDegree_ij,t-1, NetDistanceij,t-1)\n",
    "3. zi (feature)\n",
    "'''\n",
    "\n",
    "print(input_train.shape, input_test.shape, input_real.shape) # (n_samples, moment_dim), (n_samples, moment_dim), (moment_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.13659896e+00,  2.00005837e+00,  3.34075724e-03,  1.36121067e+00,\n",
       "        2.56367261e+00,  6.28272251e-03,  1.54201513e+00,  3.07818296e+00,\n",
       "        7.97219951e-03,  1.71007567e+00,  3.65867586e+00,  8.71137410e-03,\n",
       "        5.36497314e-04,  9.72694425e-01,  1.32381622e+00,  5.36205289e-04,\n",
       "       -2.53090945e-04,  7.14624277e-04,  2.61605953e-03, -2.68053329e-02,\n",
       "        7.18957948e-01,  8.24175523e-03,  9.68878036e-01,  1.33659432e+00,\n",
       "        8.17282901e-03, -3.85616901e-03,  1.08428928e-02,  4.31468626e-03,\n",
       "       -3.15861976e-02,  7.34796328e-01,  2.65586587e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 133.5827, Val Loss: 114.0630\n",
      "Epoch 2/200, Train Loss: 77.7617, Val Loss: 341.9515\n",
      "Epoch 3/200, Train Loss: 78.4388, Val Loss: 65.8587\n",
      "Epoch 4/200, Train Loss: 59.3914, Val Loss: 50.5181\n",
      "Epoch 5/200, Train Loss: 74.1005, Val Loss: 47.6608\n",
      "Epoch 6/200, Train Loss: 22.7167, Val Loss: 42.6580\n",
      "Epoch 7/200, Train Loss: 7.0540, Val Loss: 40.3200\n",
      "Epoch 8/200, Train Loss: 57.6607, Val Loss: 38.3525\n",
      "Epoch 9/200, Train Loss: 23.6220, Val Loss: 39.3016\n",
      "Epoch 10/200, Train Loss: 33.8099, Val Loss: 40.3671\n",
      "Epoch 11/200, Train Loss: 0.2291, Val Loss: 38.9996\n",
      "Epoch 12/200, Train Loss: 25.6668, Val Loss: 36.6207\n",
      "Epoch 13/200, Train Loss: 36.5096, Val Loss: 42.0711\n",
      "Epoch 14/200, Train Loss: 37.9546, Val Loss: 36.9787\n",
      "Epoch 15/200, Train Loss: 7.7854, Val Loss: 43.7293\n",
      "Epoch 16/200, Train Loss: 20.2997, Val Loss: 46.7072\n",
      "Epoch 17/200, Train Loss: 30.7493, Val Loss: 49.1016\n",
      "Epoch 18/200, Train Loss: 40.8090, Val Loss: 43.2020\n",
      "Epoch 19/200, Train Loss: 52.7868, Val Loss: 38.8965\n",
      "Epoch 20/200, Train Loss: 72.5331, Val Loss: 46.7042\n",
      "Epoch 21/200, Train Loss: 59.7429, Val Loss: 40.2520\n",
      "Epoch 22/200, Train Loss: 7.7660, Val Loss: 40.8249\n",
      "Epoch 23/200, Train Loss: 38.5799, Val Loss: 45.3814\n",
      "Epoch 24/200, Train Loss: 15.7995, Val Loss: 37.2489\n",
      "Epoch 25/200, Train Loss: 28.8934, Val Loss: 41.4350\n",
      "Epoch 26/200, Train Loss: 7.8945, Val Loss: 38.9225\n",
      "Epoch 27/200, Train Loss: 25.5933, Val Loss: 33.4847\n",
      "Epoch 28/200, Train Loss: 60.2915, Val Loss: 40.3736\n",
      "Epoch 29/200, Train Loss: 24.3770, Val Loss: 39.9529\n",
      "Epoch 30/200, Train Loss: 15.0796, Val Loss: 36.3601\n",
      "Epoch 31/200, Train Loss: 19.4159, Val Loss: 36.9583\n",
      "Epoch 32/200, Train Loss: 46.4147, Val Loss: 39.1352\n",
      "Epoch 33/200, Train Loss: 6.7183, Val Loss: 37.5878\n",
      "Epoch 34/200, Train Loss: 61.9689, Val Loss: 34.6499\n",
      "Epoch 35/200, Train Loss: 36.1883, Val Loss: 33.9243\n",
      "Epoch 36/200, Train Loss: 26.5705, Val Loss: 31.5025\n",
      "Epoch 37/200, Train Loss: 44.8806, Val Loss: 33.1280\n",
      "Epoch 38/200, Train Loss: 30.2839, Val Loss: 34.4621\n",
      "Epoch 39/200, Train Loss: 24.0983, Val Loss: 37.7991\n",
      "Epoch 40/200, Train Loss: 59.1360, Val Loss: 36.2669\n",
      "Epoch 41/200, Train Loss: 31.5791, Val Loss: 34.8823\n",
      "Epoch 42/200, Train Loss: -2.4402, Val Loss: 31.9972\n",
      "Epoch 43/200, Train Loss: 32.1857, Val Loss: 33.0274\n",
      "Epoch 44/200, Train Loss: 42.3005, Val Loss: 33.0425\n",
      "Epoch 45/200, Train Loss: 49.7090, Val Loss: 31.8323\n",
      "Epoch 46/200, Train Loss: 3.6265, Val Loss: 33.0383\n",
      "Epoch 47/200, Train Loss: 7.3165, Val Loss: 32.7163\n",
      "Epoch 48/200, Train Loss: 16.7047, Val Loss: 31.8621\n",
      "Epoch 49/200, Train Loss: 17.2469, Val Loss: 32.3946\n",
      "Epoch 50/200, Train Loss: 2.6822, Val Loss: 32.1306\n",
      "Epoch 51/200, Train Loss: 17.9643, Val Loss: 32.2699\n",
      "Epoch 52/200, Train Loss: 16.3472, Val Loss: 32.4714\n",
      "Epoch 53/200, Train Loss: 38.6189, Val Loss: 32.1132\n",
      "Epoch 54/200, Train Loss: 34.7250, Val Loss: 31.8234\n",
      "Epoch 55/200, Train Loss: -0.3390, Val Loss: 31.7821\n",
      "Epoch 56/200, Train Loss: 27.7788, Val Loss: 31.9784\n",
      "Epoch 57/200, Train Loss: 35.3293, Val Loss: 31.5367\n",
      "Epoch 58/200, Train Loss: 5.7262, Val Loss: 32.1214\n",
      "Epoch 59/200, Train Loss: 23.8154, Val Loss: 31.7148\n",
      "Epoch 60/200, Train Loss: 15.4079, Val Loss: 32.3187\n",
      "Epoch 61/200, Train Loss: 45.6538, Val Loss: 31.5798\n",
      "Epoch 62/200, Train Loss: 15.8384, Val Loss: 32.5826\n",
      "Epoch 63/200, Train Loss: 22.8032, Val Loss: 31.9449\n",
      "Epoch 64/200, Train Loss: 37.1762, Val Loss: 31.9508\n",
      "Epoch 65/200, Train Loss: 29.8160, Val Loss: 32.7056\n",
      "Epoch 66/200, Train Loss: 5.4345, Val Loss: 32.2784\n",
      "Epoch 67/200, Train Loss: 29.5794, Val Loss: 31.9127\n",
      "Epoch 68/200, Train Loss: 42.4163, Val Loss: 32.9004\n",
      "Epoch 69/200, Train Loss: 11.1567, Val Loss: 32.4760\n",
      "Epoch 70/200, Train Loss: 10.8994, Val Loss: 31.9359\n",
      "Epoch 71/200, Train Loss: 19.3348, Val Loss: 32.6772\n",
      "Epoch 72/200, Train Loss: 42.1639, Val Loss: 31.5172\n",
      "Epoch 73/200, Train Loss: 40.1566, Val Loss: 32.6472\n",
      "Epoch 74/200, Train Loss: 31.7814, Val Loss: 32.6480\n",
      "Epoch 75/200, Train Loss: 5.5437, Val Loss: 32.6279\n",
      "Epoch 76/200, Train Loss: 62.7189, Val Loss: 32.3814\n",
      "Epoch 77/200, Train Loss: 21.9792, Val Loss: 33.4744\n",
      "Epoch 78/200, Train Loss: 11.2413, Val Loss: 32.0091\n",
      "Epoch 79/200, Train Loss: 15.7931, Val Loss: 32.0949\n",
      "Epoch 80/200, Train Loss: 35.6967, Val Loss: 32.6239\n",
      "Epoch 81/200, Train Loss: 17.5645, Val Loss: 32.3099\n",
      "Epoch 82/200, Train Loss: 17.1264, Val Loss: 32.0993\n",
      "Epoch 83/200, Train Loss: 24.5196, Val Loss: 32.2744\n",
      "Epoch 84/200, Train Loss: 35.6160, Val Loss: 32.0669\n",
      "Epoch 85/200, Train Loss: 17.5166, Val Loss: 32.3575\n",
      "Epoch 86/200, Train Loss: 26.3339, Val Loss: 32.4927\n",
      "Epoch 87/200, Train Loss: 41.4269, Val Loss: 32.3598\n",
      "Epoch 88/200, Train Loss: 15.7002, Val Loss: 31.9850\n",
      "Epoch 89/200, Train Loss: 16.4230, Val Loss: 32.0944\n",
      "Epoch 90/200, Train Loss: 60.2454, Val Loss: 32.0976\n",
      "Epoch 91/200, Train Loss: 38.8333, Val Loss: 31.5330\n",
      "Epoch 92/200, Train Loss: 27.3443, Val Loss: 31.7281\n",
      "Epoch 93/200, Train Loss: 3.8812, Val Loss: 31.7336\n",
      "Epoch 94/200, Train Loss: 15.3631, Val Loss: 31.6183\n",
      "Epoch 95/200, Train Loss: 34.5561, Val Loss: 31.6642\n",
      "Epoch 96/200, Train Loss: 35.2519, Val Loss: 31.4852\n",
      "Epoch 97/200, Train Loss: 17.6063, Val Loss: 31.4298\n",
      "Epoch 98/200, Train Loss: 8.0865, Val Loss: 31.7370\n",
      "Epoch 99/200, Train Loss: 2.0918, Val Loss: 31.9931\n",
      "Epoch 100/200, Train Loss: 15.8973, Val Loss: 31.9987\n",
      "Epoch 101/200, Train Loss: 17.2154, Val Loss: 32.1407\n",
      "Epoch 102/200, Train Loss: 54.4982, Val Loss: 32.2227\n",
      "Epoch 103/200, Train Loss: 9.8585, Val Loss: 32.0286\n",
      "Epoch 104/200, Train Loss: 15.3185, Val Loss: 32.1397\n",
      "Epoch 105/200, Train Loss: 37.9989, Val Loss: 32.0489\n",
      "Epoch 106/200, Train Loss: 22.5803, Val Loss: 32.1090\n",
      "Epoch 107/200, Train Loss: 17.4291, Val Loss: 31.7700\n",
      "Epoch 108/200, Train Loss: 15.0954, Val Loss: 31.9810\n",
      "Epoch 109/200, Train Loss: 33.3087, Val Loss: 32.0169\n",
      "Epoch 110/200, Train Loss: 5.2022, Val Loss: 31.7601\n",
      "Epoch 111/200, Train Loss: 42.6701, Val Loss: 31.9772\n",
      "Epoch 112/200, Train Loss: 25.2471, Val Loss: 32.3298\n",
      "Epoch 113/200, Train Loss: 7.3423, Val Loss: 32.0078\n",
      "Epoch 114/200, Train Loss: 41.5003, Val Loss: 31.9246\n",
      "Epoch 115/200, Train Loss: 17.6830, Val Loss: 31.9246\n",
      "Epoch 116/200, Train Loss: 8.9695, Val Loss: 31.8424\n",
      "Epoch 117/200, Train Loss: 17.5405, Val Loss: 31.8056\n",
      "Epoch 118/200, Train Loss: 7.0188, Val Loss: 31.7117\n",
      "Epoch 119/200, Train Loss: 4.0550, Val Loss: 32.0029\n",
      "Epoch 120/200, Train Loss: -0.3117, Val Loss: 31.8499\n",
      "Epoch 121/200, Train Loss: 23.9629, Val Loss: 31.8419\n",
      "Epoch 122/200, Train Loss: 9.8052, Val Loss: 31.8425\n",
      "Epoch 123/200, Train Loss: 14.8753, Val Loss: 31.8452\n",
      "Epoch 124/200, Train Loss: 59.6467, Val Loss: 31.8357\n",
      "Epoch 125/200, Train Loss: 69.0241, Val Loss: 31.8347\n",
      "Epoch 126/200, Train Loss: 26.0282, Val Loss: 31.8267\n",
      "Epoch 127/200, Train Loss: 39.2383, Val Loss: 31.8143\n",
      "Epoch 128/200, Train Loss: 22.2494, Val Loss: 31.8269\n",
      "Epoch 129/200, Train Loss: 22.8609, Val Loss: 31.8537\n",
      "Epoch 130/200, Train Loss: 87.5677, Val Loss: 31.8552\n",
      "Epoch 131/200, Train Loss: 28.7860, Val Loss: 31.8596\n",
      "Epoch 132/200, Train Loss: 1.0943, Val Loss: 31.8404\n",
      "Epoch 133/200, Train Loss: 16.4190, Val Loss: 31.8383\n",
      "Epoch 134/200, Train Loss: 11.6754, Val Loss: 31.8119\n",
      "Epoch 135/200, Train Loss: 12.0867, Val Loss: 31.8095\n",
      "Epoch 136/200, Train Loss: 23.5945, Val Loss: 31.8300\n",
      "Epoch 137/200, Train Loss: 27.3843, Val Loss: 31.7955\n",
      "Epoch 138/200, Train Loss: 11.4089, Val Loss: 31.7569\n",
      "Epoch 139/200, Train Loss: 27.6809, Val Loss: 31.7310\n",
      "Epoch 140/200, Train Loss: 30.6534, Val Loss: 31.7635\n",
      "Epoch 141/200, Train Loss: 11.7347, Val Loss: 31.8005\n",
      "Epoch 142/200, Train Loss: 5.7950, Val Loss: 31.7846\n",
      "Epoch 143/200, Train Loss: 39.1973, Val Loss: 31.7773\n",
      "Epoch 144/200, Train Loss: 21.2128, Val Loss: 31.8115\n",
      "Epoch 145/200, Train Loss: 19.1305, Val Loss: 31.7912\n",
      "Epoch 146/200, Train Loss: 7.6575, Val Loss: 31.8205\n",
      "Epoch 147/200, Train Loss: 7.4454, Val Loss: 31.8071\n",
      "Epoch 148/200, Train Loss: 7.2860, Val Loss: 31.8081\n",
      "Epoch 149/200, Train Loss: 28.3679, Val Loss: 31.8074\n",
      "Epoch 150/200, Train Loss: -0.8063, Val Loss: 31.7897\n",
      "Epoch 151/200, Train Loss: -2.7779, Val Loss: 31.7796\n",
      "Epoch 152/200, Train Loss: 39.4791, Val Loss: 31.8140\n",
      "Epoch 153/200, Train Loss: 21.0622, Val Loss: 31.7652\n",
      "Epoch 154/200, Train Loss: 15.7732, Val Loss: 31.7278\n",
      "Epoch 155/200, Train Loss: 5.3152, Val Loss: 31.7132\n",
      "Epoch 156/200, Train Loss: 12.0423, Val Loss: 31.7129\n",
      "Epoch 157/200, Train Loss: 9.4772, Val Loss: 31.7148\n",
      "Epoch 158/200, Train Loss: 32.8757, Val Loss: 31.7449\n",
      "Epoch 159/200, Train Loss: 20.5404, Val Loss: 31.8014\n",
      "Epoch 160/200, Train Loss: 57.9989, Val Loss: 31.8304\n",
      "Epoch 161/200, Train Loss: 21.8766, Val Loss: 31.8340\n",
      "Epoch 162/200, Train Loss: 7.8705, Val Loss: 31.8380\n",
      "Epoch 163/200, Train Loss: 9.5140, Val Loss: 31.8371\n",
      "Epoch 164/200, Train Loss: 30.8373, Val Loss: 31.8352\n",
      "Epoch 165/200, Train Loss: 14.5687, Val Loss: 31.8370\n",
      "Epoch 166/200, Train Loss: 30.7995, Val Loss: 31.8410\n",
      "Epoch 167/200, Train Loss: 14.9440, Val Loss: 31.8451\n",
      "Epoch 168/200, Train Loss: 25.5204, Val Loss: 31.8476\n",
      "Epoch 169/200, Train Loss: 15.0210, Val Loss: 31.8475\n",
      "Epoch 170/200, Train Loss: 10.2776, Val Loss: 31.8456\n",
      "Epoch 171/200, Train Loss: 20.4075, Val Loss: 31.8441\n",
      "Epoch 172/200, Train Loss: 15.4502, Val Loss: 31.8412\n",
      "Epoch 173/200, Train Loss: 13.0308, Val Loss: 31.8415\n",
      "Epoch 174/200, Train Loss: 42.1007, Val Loss: 31.8431\n",
      "Epoch 175/200, Train Loss: 17.2316, Val Loss: 31.8437\n",
      "Epoch 176/200, Train Loss: 17.9906, Val Loss: 31.8409\n",
      "Epoch 177/200, Train Loss: 35.9332, Val Loss: 31.8382\n",
      "Epoch 178/200, Train Loss: 9.5159, Val Loss: 31.8352\n",
      "Epoch 179/200, Train Loss: 21.9542, Val Loss: 31.8345\n",
      "Epoch 180/200, Train Loss: 23.5512, Val Loss: 31.8382\n",
      "Epoch 181/200, Train Loss: 45.1889, Val Loss: 31.8415\n",
      "Epoch 182/200, Train Loss: 39.4726, Val Loss: 31.8509\n",
      "Epoch 183/200, Train Loss: 9.9549, Val Loss: 31.8524\n",
      "Epoch 184/200, Train Loss: 16.3735, Val Loss: 31.8526\n",
      "Epoch 185/200, Train Loss: 33.3047, Val Loss: 31.8493\n",
      "Epoch 186/200, Train Loss: 45.7972, Val Loss: 31.8540\n",
      "Epoch 187/200, Train Loss: 15.2111, Val Loss: 31.8557\n",
      "Epoch 188/200, Train Loss: 25.3664, Val Loss: 31.8561\n",
      "Epoch 189/200, Train Loss: 34.7193, Val Loss: 31.8616\n",
      "Epoch 190/200, Train Loss: 39.3481, Val Loss: 31.8649\n",
      "Epoch 191/200, Train Loss: 1.8455, Val Loss: 31.8597\n",
      "Epoch 192/200, Train Loss: 11.9927, Val Loss: 31.8599\n",
      "Epoch 193/200, Train Loss: 69.2983, Val Loss: 31.8613\n",
      "Epoch 194/200, Train Loss: 6.4820, Val Loss: 31.8676\n",
      "Epoch 195/200, Train Loss: 32.6284, Val Loss: 31.8656\n",
      "Epoch 196/200, Train Loss: 4.9758, Val Loss: 31.8594\n",
      "Epoch 197/200, Train Loss: 3.7566, Val Loss: 31.8585\n",
      "Epoch 198/200, Train Loss: 4.1438, Val Loss: 31.8571\n",
      "Epoch 199/200, Train Loss: 31.2488, Val Loss: 31.8555\n",
      "Epoch 200/200, Train Loss: 13.2913, Val Loss: 31.8529\n",
      "Test results:                    bias         rmse      mean_SD\n",
      "\\lambda_f   0.037 (0.1)  0.770 (0.1)  0.465 (0.0)\n",
      "\\alpha_f   -0.511 (0.1)  1.161 (0.1)  0.681 (0.0)\n",
      "\\beta_f     0.015 (0.1)  0.525 (0.0)  0.907 (0.0)\n",
      "\\delta_f    0.178 (0.2)  2.374 (0.2)  0.408 (0.0)\n",
      "\\theta_f    0.027 (0.0)  0.185 (0.0)  0.427 (0.0)\n",
      "\\gamma_f   -0.085 (0.0)  0.244 (0.0)  0.461 (0.0)\n",
      "\\tau       -0.073 (0.1)  1.081 (0.1)  0.640 (0.0)\n",
      "[-5.9075489   9.2110405   0.77698827  3.42420888  0.63840091  0.61024439\n",
      "  2.71588516]\n",
      "[0.5098519  0.7983891  0.8271841  0.3964754  0.3915194  0.49288756\n",
      " 0.6305784 ]\n"
     ]
    }
   ],
   "source": [
    "R_train, R_test = input_train.shape[0], input_test.shape[0] # num of train/test 'samples'\n",
    "R = R_train + R_test\n",
    "M, L = input_train.shape[1], label_train.shape[1]\n",
    "\n",
    "if args.learn_standard_error:   # sd as labels\n",
    "    label_train = np.hstack([label_train, np.zeros((R_train, L))])\n",
    "    label_test = np.hstack([label_test, np.zeros((R_test, L))])\n",
    "    output_dim = 2 * L\n",
    "else:\n",
    "    output_dim = L\n",
    "\n",
    "scaler = StandardScaler()\n",
    "input_train, input_test = scaler.fit_transform(input_train), scaler.fit_transform(input_test)\n",
    "input_real = scaler.fit_transform(input_real.reshape(1, -1))\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(input_train, dtype=torch.float32),\n",
    "                              torch.tensor(label_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "net = NeuralNet(M, args.num_nodes, output_dim)\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "net.to(device)\n",
    "if args.learn_standard_error:\n",
    "    criterion = forward_loss\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.initial_lr)\n",
    "\n",
    "# Optional: learning rate scheduler similar to MATLAB piecewise schedule\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1) # 40, 0.1\n",
    "\n",
    "for epoch in range(args.max_epochs):\n",
    "    net.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Validation / Test summary (optional)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = net(torch.tensor(input_test, dtype=torch.float32))\n",
    "        loss_va = criterion(test_preds, torch.tensor(label_test, dtype=torch.float32))\n",
    "\n",
    "    if args.disp_iter:\n",
    "        print(f\"Epoch {epoch + 1}/{args.max_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {loss_va.item():.4f}\")\n",
    "\n",
    "\n",
    "if args.disp_test_summary:\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = net(torch.tensor(input_test, dtype=torch.float32))\n",
    "        Test_error_summary(torch.tensor(input_test, dtype=torch.float32), label_test, label_name, net, figure=args.display_fig, table=1)\n",
    "\n",
    "# Estimate on original data\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    temp = net(torch.tensor(input_real.squeeze(0), dtype=torch.float32)).numpy()\n",
    "\n",
    "# 截断 theta\n",
    "theta = np.clip(temp[:L], lb, ub)\n",
    "\n",
    "# 正向变换标准误（如果学习）\n",
    "if args.learn_standard_error:\n",
    "    se = Positive_transform(temp[L:2 * L])\n",
    "\n",
    "print(theta)\n",
    "print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
